apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    description: vLLM ServingRuntime to support IBM Spyre
    internal.config.kubernetes.io/previousKinds: Template
    internal.config.kubernetes.io/previousNames: vllm-spyre-runtime-template
    internal.config.kubernetes.io/previousNamespaces: opendatahub
    opendatahub.io/apiProtocol: REST
    opendatahub.io/model-type: '["generative"]'
    opendatahub.io/modelServingSupport: '["single"]'
    openshift.io/display-name: vLLM Spyre AI Accelerator ServingRuntime for KServe
    openshift.io/provider-display-name: Red Hat, Inc.
    platform.opendatahub.io/instance.generation: "1"
    platform.opendatahub.io/instance.name: default-modelcontroller
    platform.opendatahub.io/instance.uid: 9d8d9a22-2823-4529-897d-17e27f96baae
    platform.opendatahub.io/type: OpenShift AI Self-Managed
    platform.opendatahub.io/version: 2.25.0
    tags: rhods,rhoai,kserve,servingruntime
    template.openshift.io/documentation-url: https://github.com/opendatahub-io/vllm
    template.openshift.io/long-description: This template defines resources needed
      to deploy vLLM Spyre AI Accelerator ServingRuntime with KServe in Red Hat OpenShift
      AI
  labels:
    app: odh-dashboard
    app.kubernetes.io/part-of: odh-model-controller
    app.opendatahub.io/odh-model-controller: "true"
    opendatahub.io/dashboard: "true"
    opendatahub.io/ootb: "true"
    platform.opendatahub.io/part-of: modelcontroller
  name: vllm-spyre-runtime-template
  namespace: redhat-ods-applications
objects:
- apiVersion: serving.kserve.io/v1alpha1
  kind: ServingRuntime
  metadata:
    annotations:
      opendatahub.io/recommended-accelerators: '["ibm.com/spyre_pf"]'
      opendatahub.io/runtime-version: v0.10.1.1
      openshift.io/display-name: vLLM Spyre AI Accelerator ServingRuntime for KServe
    labels:
      opendatahub.io/dashboard: "true"
    name: vllm-spyre-runtime
  spec:
    annotations:
      prometheus.io/path: /metrics
      prometheus.io/port: "8080"
    containers:
    - args:
      - /mnt/models
      - --port=8000
      - --served-model-name={{.Name}}
      env:
      - name: HF_HOME
        value: /tmp/hf_home
      - name: FLEX_COMPUTE
        value: SENTIENT
      - name: FLEX_DEVICE
        value: PF
      - name: TOKENIZERS_PARALLELISM
        value: "false"
      - name: DTLOG_LEVEL
        value: error
      - name: TORCH_SENDNN_LOG
        value: CRITICAL
      - name: VLLM_SPYRE_WARMUP_BATCH_SIZES
        value: "4"
      - name: VLLM_SPYRE_WARMUP_PROMPT_LENS
        value: "1024"
      - name: VLLM_SPYRE_WARMUP_NEW_TOKENS
        value: "256"
      image: registry.redhat.io/rhaiis/vllm-spyre-rhel9@sha256:f36864ec0d7a18a3143296ba2e624eca1fa6ba0af87a7196b325384785bbcf4b
      name: kserve-container
      ports:
      - containerPort: 8000
        protocol: TCP
    multiModel: false
    supportedModelFormats:
    - autoSelect: true
      name: vLLM
